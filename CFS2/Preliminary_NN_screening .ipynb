{"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Preliminary_neural_network_screening.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8414138,"sourceType":"datasetVersion","datasetId":5008220}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Deployed in Colabs with TPU accelerator","metadata":{"id":"40jLgWPlMKxn"}},{"cell_type":"code","source":"# import the necessary libraries to execute this code\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import GroupKFold, GroupShuffleSplit\nfrom sklearn.model_selection import RandomizedSearchCV as RSCV\n\n# build NN for class\nfrom tensorflow.keras.layers import  Dropout\nfrom tensorflow.keras.regularizers import l1_l2\nfrom tensorflow.keras.layers import Dense,Dropout\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n#from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n","metadata":{"id":"bf20428e","execution":{"iopub.status.busy":"2024-05-15T20:09:44.839876Z","iopub.execute_input":"2024-05-15T20:09:44.840308Z","iopub.status.idle":"2024-05-15T20:09:44.886464Z","shell.execute_reply.started":"2024-05-15T20:09:44.840272Z","shell.execute_reply":"2024-05-15T20:09:44.885169Z"},"trusted":true},"execution_count":12,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscikeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasRegressor\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scikeras'"],"ename":"ModuleNotFoundError","evalue":"No module named 'scikeras'","output_type":"error"}]},{"cell_type":"markdown","source":"# Data Processing","metadata":{}},{"cell_type":"code","source":"\ndf=pd.read_excel('/kaggle/input/cfs-plasma-proteins/Plasma_for_CFS.xlsx')\ninputDf=df.iloc[55:112,2:1348]\n\ntitles=np.concatenate((df.iloc[54,2:31],df.iloc[45,31:1348]),axis=None)\ninputDf.columns=titles\n\ninputDf=inputDf[inputDf['SampleType']==\"Sample\"]\ninputDf.index=range(0,len(inputDf.index)) # update row numbers\n\nyArr=(inputDf[\"SampleGroup\"]==\"Patient\").astype(int) # patient is 1, control is 0\n\nxArr=pd.concat([(inputDf[\"Gender\"]==\"F\").astype(int),inputDf[\"Age\"],inputDf.iloc[0:57,29:1346]], axis=1)\nfeatureTitles=xArr.columns\n\ndisplay(xArr)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T20:07:13.984345Z","iopub.execute_input":"2024-05-15T20:07:13.984709Z","iopub.status.idle":"2024-05-15T20:07:16.484218Z","shell.execute_reply.started":"2024-05-15T20:07:13.984682Z","shell.execute_reply":"2024-05-15T20:07:16.483214Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"    Gender Age    CHIP   CEBPB    NSE   PIAS4 IL-10 Ra  STAT3    IRF1   c-Jun  \\\n0        1  25   596.8  1277.7  622.1   689.1   1825.1  415.8   791.3  2539.1   \n1        0  43   673.8  1932.8  653.2   836.2   1752.5  560.6   719.1  3830.2   \n2        1  24   722.3  1509.8  643.2   923.5   1752.5  610.3     752  2319.8   \n3        1  51   583.4    2124  630.2  1126.3   1590.7  825.3     775  2487.9   \n4        0  37   790.8    1859  650.6  1028.1   1797.3  772.9   780.8  3082.4   \n5        0  22   901.3  1401.9  643.7   784.6   1670.4  508.5   687.4  3015.2   \n6        1  56   616.5  1465.9  681.5   979.7   1731.1  519.4   717.2  2656.5   \n7        1  26   786.2  1809.6  554.9  1036.8   1659.6  569.9   840.3  2841.1   \n8        1  22   828.6  1769.9  624.6   976.2   1568.5  963.4   757.1  2587.9   \n9        0  60  1271.4  1529.7    687   914.6   1651.5  450.4  1078.2  2718.7   \n10       1  37   736.1  1346.3    685   790.9   1818.7  516.3   782.2    3038   \n11       0  40  1119.9  1583.7  641.3  1001.6   1637.8    584   828.1  2932.3   \n12       0  26   660.6  1446.9  783.3   801.9   1747.6  608.6   796.7  2475.8   \n13       0  48  1113.9  1453.7  652.5   971.8   1486.4  612.4  1159.6  2343.4   \n14       1  50   740.8  1774.7  577.2  1067.1   1583.6  712.4   766.8    2850   \n15       0  47   945.1  1904.8  651.6  1050.5   1646.4  694.4   740.6  2991.7   \n16       1  54   697.9  1702.4  629.7   974.8   1711.5  509.2   737.4  3199.2   \n17       0  58   677.8  1253.6  688.6   750.3     1948  507.3   704.2  2572.5   \n18       0  60     743  1500.2  585.4  1040.6   1799.1  463.1   835.6  2541.1   \n19       0  37   928.8  1863.1  772.8  1133.9   1680.7  511.9     910  2970.6   \n20       0  33   900.1  1634.2  637.6   956.8     1785  500.3   760.6  2850.9   \n21       0  58   864.1  2405.4    639    1066   1595.8  988.7   772.8  2152.8   \n22       0  47   679.9    1500  620.4   694.9   1651.7  744.7   718.5  2276.6   \n23       1  61  1162.2   53296  601.8   994.2   1474.2  538.4   717.4  2502.5   \n24       1  49   709.7  1482.4  626.6   868.2   1577.9  581.4   728.6  2578.3   \n25       1  60   788.5  1851.1  642.4   988.8   1627.8  481.6   783.4  2539.7   \n26       0  36   897.3  1449.2  627.9   889.8   1638.4  564.1   812.6  2738.4   \n27       1  20   837.7  1834.8  624.6  1117.5   1621.6  636.9   777.7  3005.5   \n28       1  25   700.8  1472.5    577   786.1   1721.4  565.4   856.9  2410.6   \n29       1  44   969.6  1874.2  742.3  1463.5   2034.3    355   879.7  3275.4   \n30       1  55  1300.5  1779.2  594.5  1086.7   1703.1  483.2     815  2972.1   \n31       1  54   714.3  1585.6  613.4   790.9     1744  531.9   725.3  2556.4   \n32       1  61   858.9  3104.6    617   670.6   1632.9  698.4   692.9  2403.1   \n33       0  50   841.2  1508.9  707.6   975.1   1639.9  622.7   796.6  2637.8   \n34       1  23     773  3024.4  591.8  1278.2   1610.7  876.9   808.9  3087.8   \n35       1  56   852.7    1313  661.5   720.4   1806.2  468.7   727.4  2516.1   \n36       1  57   649.6  1724.4  624.5  1025.8   1721.8    505   763.9  2858.1   \n37       0  58   650.2    1617  614.4   797.9   1612.5  622.2   723.8  2502.2   \n38       1  38   609.5  1388.3  662.7     910   1774.1  482.9   758.7  2526.7   \n39       0  23  1025.2  1962.9    764  1088.3   1548.9    728  1052.4  2730.1   \n40       0  44   849.7  1431.6  547.9   819.9   1608.7  539.3   767.7  2252.1   \n41       1  56  1189.4  1612.9  646.6   923.6   1585.6  469.7   803.1  2530.2   \n\n    ...   UB2G2 Transgelin-2    ATPO Corticotropin-lipotropin   QORL1  \\\n0   ...  6233.7       3864.8  1474.8                    952.1  1923.5   \n1   ...  6218.3         3896  4185.5                   1703.5  1791.1   \n2   ...  5335.7       3877.8  1542.9                   1193.8  1912.5   \n3   ...  5763.3       4219.6  3029.7                    775.2  2642.1   \n4   ...    5904       4369.3  2461.9                   2201.7  1532.6   \n5   ...  5166.2         4737  2044.6                   1563.6  1509.8   \n6   ...  7101.6       5593.5  1519.9                   1176.6  1913.8   \n7   ...    6314       4706.9  1778.7                   1063.1  1612.1   \n8   ...  4841.9       5338.7  1962.1                    901.8  1781.2   \n9   ...  5716.6       5073.2  1827.4                     1158  1750.5   \n10  ...  6406.5       4366.9  1439.1                   1152.8  1685.8   \n11  ...  5975.1       4563.7  2252.2                   1514.5    1764   \n12  ...  6478.7       4667.3  2038.9                   2214.9  1840.2   \n13  ...  6007.9       4151.8  1871.6                      989  1947.7   \n14  ...  5477.6       4064.4  1719.4                    990.3  2608.2   \n15  ...    6470       5040.7  1915.9                    987.7  1546.4   \n16  ...  6764.1       4009.2  1625.4                   1145.6  2402.5   \n17  ...  6034.1         5914  1929.2                   1342.7  1799.8   \n18  ...  6467.4       4185.6    1874                   3359.3  1880.5   \n19  ...  5923.6      10886.3  1852.3                   1297.9  2176.9   \n20  ...  5605.2       3474.9  1714.7                   1703.5  1969.5   \n21  ...  6056.9       6305.9  2083.7                   1119.5  1857.5   \n22  ...  5916.9       3472.5  1649.8                   1044.3  1778.9   \n23  ...  7288.2       3786.9  1457.8                   1583.4  1700.9   \n24  ...  6081.3       3703.1    1488                    990.7  1771.7   \n25  ...  7812.4       4276.3  1421.4                   1218.3  2874.3   \n26  ...  6172.8       3961.7  2082.8                   1090.8  2097.9   \n27  ...  5988.3       9247.1  2016.1                   1481.9    1887   \n28  ...  5619.7         3805  1560.7                   1235.4  1752.2   \n29  ...  8102.4       5301.9  2122.5                   1309.5  2482.7   \n30  ...  7766.7       4094.5  1591.8                   1293.6  2905.1   \n31  ...  6235.2       7875.5    1788                   1166.5  1734.4   \n32  ...  5100.5         4934  2671.3                    993.3    1657   \n33  ...  6172.2         4471  2001.1                   1098.5  2688.2   \n34  ...    5618         4033  1820.5                    680.1  2156.1   \n35  ...  6582.5       3745.9  1578.6                   1473.2    1662   \n36  ...  6752.4       3837.5  1771.3                   1022.3    2250   \n37  ...  6249.1       4355.9  1701.2                    907.1  1951.4   \n38  ...  7033.5       4095.3  1573.3                     1098  2186.5   \n39  ...  6593.1       4653.6  1908.3                   1251.7  1453.6   \n40  ...  5820.4       4006.9  1722.6                     1418  1538.3   \n41  ...  5957.5         3826  2090.9                   1462.9    1576   \n\n       PEDF    CATF      FTCD   UBP25   PLXB2  \n0   38280.5  3298.5    8208.4  1931.4  4597.4  \n1   50142.8  3527.2    8245.3  2095.9  3399.3  \n2   35329.8  2922.3    1353.8  1669.3  3739.6  \n3   39961.3  5114.1    3677.9  1521.1  4053.5  \n4   43351.6  3077.7    3900.4  1914.8  3839.1  \n5   48178.9  2332.4    4078.9  1759.5    4002  \n6   53541.1  3440.3   26390.8  1816.2  2690.2  \n7   37400.9  3420.5    3754.2  1711.2  3284.4  \n8   43965.5  2870.2    1707.5  1923.1  2937.5  \n9   43568.1  4230.4    4197.8  1666.7  4105.1  \n10  50201.9  3523.4    2789.9  1896.8  3955.3  \n11  47744.8  2974.6    1426.7  1675.9  3209.8  \n12  46131.1  3219.2    2366.6  1766.8  3788.3  \n13    42132  2812.9   13319.6  2696.4  4000.1  \n14  36963.1  4579.3    6782.4  1811.1  3382.2  \n15  48295.4  3135.6    6546.7  1874.2  3649.4  \n16  48435.2  3778.1    9605.9  1885.8  3186.2  \n17  45860.9  2672.2    3800.4  1750.3  3550.3  \n18  52718.7  3097.6    4902.2  1748.5  4637.3  \n19  38596.4  3519.6      3952  1704.6  3776.5  \n20  42356.8  3733.1    2285.8  1694.6  4222.4  \n21  49424.6  2652.5    5989.3  1520.8  3392.1  \n22  44215.9  3641.3    4177.6  1721.1  3827.5  \n23  45366.5    3757   21388.2  1863.4  5451.3  \n24  45773.7  2360.2   21346.6  1855.4  3633.7  \n25    56581  4822.7   48783.8  2026.7  5097.2  \n26  54538.3    2898   17633.6  1726.3  3608.7  \n27  39724.2  2677.1    3087.8  1818.9  2859.6  \n28  40213.1  2377.3      4907  1610.4  3660.6  \n29    35540  4436.2    2806.8  1858.7  4646.6  \n30  46553.6  3333.2    8698.4  1814.7  3545.6  \n31  56901.8    2933    3095.8  1647.7  3636.7  \n32  48410.6  3788.2  142773.7  1790.3  4445.6  \n33  44163.6  3242.4    7225.3  2030.3  3743.1  \n34  41531.7  4145.6   19229.4  1977.3  2998.3  \n35  51701.8  3254.9    3779.2  2227.7  5992.9  \n36  54806.9  4995.3    4240.4  1921.3  3950.2  \n37  58326.4  3836.4    9081.7  1655.2  3463.9  \n38  51337.6  3890.2    3423.9  1725.3  3330.7  \n39  44700.7  2814.2    2007.7  2067.9  3066.1  \n40  46719.5  4732.5      2237    1527  3751.5  \n41  45429.9    3838   12676.7  1950.6  4877.9  \n\n[42 rows x 1319 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>CHIP</th>\n      <th>CEBPB</th>\n      <th>NSE</th>\n      <th>PIAS4</th>\n      <th>IL-10 Ra</th>\n      <th>STAT3</th>\n      <th>IRF1</th>\n      <th>c-Jun</th>\n      <th>...</th>\n      <th>UB2G2</th>\n      <th>Transgelin-2</th>\n      <th>ATPO</th>\n      <th>Corticotropin-lipotropin</th>\n      <th>QORL1</th>\n      <th>PEDF</th>\n      <th>CATF</th>\n      <th>FTCD</th>\n      <th>UBP25</th>\n      <th>PLXB2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>25</td>\n      <td>596.8</td>\n      <td>1277.7</td>\n      <td>622.1</td>\n      <td>689.1</td>\n      <td>1825.1</td>\n      <td>415.8</td>\n      <td>791.3</td>\n      <td>2539.1</td>\n      <td>...</td>\n      <td>6233.7</td>\n      <td>3864.8</td>\n      <td>1474.8</td>\n      <td>952.1</td>\n      <td>1923.5</td>\n      <td>38280.5</td>\n      <td>3298.5</td>\n      <td>8208.4</td>\n      <td>1931.4</td>\n      <td>4597.4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>43</td>\n      <td>673.8</td>\n      <td>1932.8</td>\n      <td>653.2</td>\n      <td>836.2</td>\n      <td>1752.5</td>\n      <td>560.6</td>\n      <td>719.1</td>\n      <td>3830.2</td>\n      <td>...</td>\n      <td>6218.3</td>\n      <td>3896</td>\n      <td>4185.5</td>\n      <td>1703.5</td>\n      <td>1791.1</td>\n      <td>50142.8</td>\n      <td>3527.2</td>\n      <td>8245.3</td>\n      <td>2095.9</td>\n      <td>3399.3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>24</td>\n      <td>722.3</td>\n      <td>1509.8</td>\n      <td>643.2</td>\n      <td>923.5</td>\n      <td>1752.5</td>\n      <td>610.3</td>\n      <td>752</td>\n      <td>2319.8</td>\n      <td>...</td>\n      <td>5335.7</td>\n      <td>3877.8</td>\n      <td>1542.9</td>\n      <td>1193.8</td>\n      <td>1912.5</td>\n      <td>35329.8</td>\n      <td>2922.3</td>\n      <td>1353.8</td>\n      <td>1669.3</td>\n      <td>3739.6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>51</td>\n      <td>583.4</td>\n      <td>2124</td>\n      <td>630.2</td>\n      <td>1126.3</td>\n      <td>1590.7</td>\n      <td>825.3</td>\n      <td>775</td>\n      <td>2487.9</td>\n      <td>...</td>\n      <td>5763.3</td>\n      <td>4219.6</td>\n      <td>3029.7</td>\n      <td>775.2</td>\n      <td>2642.1</td>\n      <td>39961.3</td>\n      <td>5114.1</td>\n      <td>3677.9</td>\n      <td>1521.1</td>\n      <td>4053.5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>37</td>\n      <td>790.8</td>\n      <td>1859</td>\n      <td>650.6</td>\n      <td>1028.1</td>\n      <td>1797.3</td>\n      <td>772.9</td>\n      <td>780.8</td>\n      <td>3082.4</td>\n      <td>...</td>\n      <td>5904</td>\n      <td>4369.3</td>\n      <td>2461.9</td>\n      <td>2201.7</td>\n      <td>1532.6</td>\n      <td>43351.6</td>\n      <td>3077.7</td>\n      <td>3900.4</td>\n      <td>1914.8</td>\n      <td>3839.1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>22</td>\n      <td>901.3</td>\n      <td>1401.9</td>\n      <td>643.7</td>\n      <td>784.6</td>\n      <td>1670.4</td>\n      <td>508.5</td>\n      <td>687.4</td>\n      <td>3015.2</td>\n      <td>...</td>\n      <td>5166.2</td>\n      <td>4737</td>\n      <td>2044.6</td>\n      <td>1563.6</td>\n      <td>1509.8</td>\n      <td>48178.9</td>\n      <td>2332.4</td>\n      <td>4078.9</td>\n      <td>1759.5</td>\n      <td>4002</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>56</td>\n      <td>616.5</td>\n      <td>1465.9</td>\n      <td>681.5</td>\n      <td>979.7</td>\n      <td>1731.1</td>\n      <td>519.4</td>\n      <td>717.2</td>\n      <td>2656.5</td>\n      <td>...</td>\n      <td>7101.6</td>\n      <td>5593.5</td>\n      <td>1519.9</td>\n      <td>1176.6</td>\n      <td>1913.8</td>\n      <td>53541.1</td>\n      <td>3440.3</td>\n      <td>26390.8</td>\n      <td>1816.2</td>\n      <td>2690.2</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td>26</td>\n      <td>786.2</td>\n      <td>1809.6</td>\n      <td>554.9</td>\n      <td>1036.8</td>\n      <td>1659.6</td>\n      <td>569.9</td>\n      <td>840.3</td>\n      <td>2841.1</td>\n      <td>...</td>\n      <td>6314</td>\n      <td>4706.9</td>\n      <td>1778.7</td>\n      <td>1063.1</td>\n      <td>1612.1</td>\n      <td>37400.9</td>\n      <td>3420.5</td>\n      <td>3754.2</td>\n      <td>1711.2</td>\n      <td>3284.4</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1</td>\n      <td>22</td>\n      <td>828.6</td>\n      <td>1769.9</td>\n      <td>624.6</td>\n      <td>976.2</td>\n      <td>1568.5</td>\n      <td>963.4</td>\n      <td>757.1</td>\n      <td>2587.9</td>\n      <td>...</td>\n      <td>4841.9</td>\n      <td>5338.7</td>\n      <td>1962.1</td>\n      <td>901.8</td>\n      <td>1781.2</td>\n      <td>43965.5</td>\n      <td>2870.2</td>\n      <td>1707.5</td>\n      <td>1923.1</td>\n      <td>2937.5</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>60</td>\n      <td>1271.4</td>\n      <td>1529.7</td>\n      <td>687</td>\n      <td>914.6</td>\n      <td>1651.5</td>\n      <td>450.4</td>\n      <td>1078.2</td>\n      <td>2718.7</td>\n      <td>...</td>\n      <td>5716.6</td>\n      <td>5073.2</td>\n      <td>1827.4</td>\n      <td>1158</td>\n      <td>1750.5</td>\n      <td>43568.1</td>\n      <td>4230.4</td>\n      <td>4197.8</td>\n      <td>1666.7</td>\n      <td>4105.1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1</td>\n      <td>37</td>\n      <td>736.1</td>\n      <td>1346.3</td>\n      <td>685</td>\n      <td>790.9</td>\n      <td>1818.7</td>\n      <td>516.3</td>\n      <td>782.2</td>\n      <td>3038</td>\n      <td>...</td>\n      <td>6406.5</td>\n      <td>4366.9</td>\n      <td>1439.1</td>\n      <td>1152.8</td>\n      <td>1685.8</td>\n      <td>50201.9</td>\n      <td>3523.4</td>\n      <td>2789.9</td>\n      <td>1896.8</td>\n      <td>3955.3</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0</td>\n      <td>40</td>\n      <td>1119.9</td>\n      <td>1583.7</td>\n      <td>641.3</td>\n      <td>1001.6</td>\n      <td>1637.8</td>\n      <td>584</td>\n      <td>828.1</td>\n      <td>2932.3</td>\n      <td>...</td>\n      <td>5975.1</td>\n      <td>4563.7</td>\n      <td>2252.2</td>\n      <td>1514.5</td>\n      <td>1764</td>\n      <td>47744.8</td>\n      <td>2974.6</td>\n      <td>1426.7</td>\n      <td>1675.9</td>\n      <td>3209.8</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0</td>\n      <td>26</td>\n      <td>660.6</td>\n      <td>1446.9</td>\n      <td>783.3</td>\n      <td>801.9</td>\n      <td>1747.6</td>\n      <td>608.6</td>\n      <td>796.7</td>\n      <td>2475.8</td>\n      <td>...</td>\n      <td>6478.7</td>\n      <td>4667.3</td>\n      <td>2038.9</td>\n      <td>2214.9</td>\n      <td>1840.2</td>\n      <td>46131.1</td>\n      <td>3219.2</td>\n      <td>2366.6</td>\n      <td>1766.8</td>\n      <td>3788.3</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0</td>\n      <td>48</td>\n      <td>1113.9</td>\n      <td>1453.7</td>\n      <td>652.5</td>\n      <td>971.8</td>\n      <td>1486.4</td>\n      <td>612.4</td>\n      <td>1159.6</td>\n      <td>2343.4</td>\n      <td>...</td>\n      <td>6007.9</td>\n      <td>4151.8</td>\n      <td>1871.6</td>\n      <td>989</td>\n      <td>1947.7</td>\n      <td>42132</td>\n      <td>2812.9</td>\n      <td>13319.6</td>\n      <td>2696.4</td>\n      <td>4000.1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1</td>\n      <td>50</td>\n      <td>740.8</td>\n      <td>1774.7</td>\n      <td>577.2</td>\n      <td>1067.1</td>\n      <td>1583.6</td>\n      <td>712.4</td>\n      <td>766.8</td>\n      <td>2850</td>\n      <td>...</td>\n      <td>5477.6</td>\n      <td>4064.4</td>\n      <td>1719.4</td>\n      <td>990.3</td>\n      <td>2608.2</td>\n      <td>36963.1</td>\n      <td>4579.3</td>\n      <td>6782.4</td>\n      <td>1811.1</td>\n      <td>3382.2</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0</td>\n      <td>47</td>\n      <td>945.1</td>\n      <td>1904.8</td>\n      <td>651.6</td>\n      <td>1050.5</td>\n      <td>1646.4</td>\n      <td>694.4</td>\n      <td>740.6</td>\n      <td>2991.7</td>\n      <td>...</td>\n      <td>6470</td>\n      <td>5040.7</td>\n      <td>1915.9</td>\n      <td>987.7</td>\n      <td>1546.4</td>\n      <td>48295.4</td>\n      <td>3135.6</td>\n      <td>6546.7</td>\n      <td>1874.2</td>\n      <td>3649.4</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1</td>\n      <td>54</td>\n      <td>697.9</td>\n      <td>1702.4</td>\n      <td>629.7</td>\n      <td>974.8</td>\n      <td>1711.5</td>\n      <td>509.2</td>\n      <td>737.4</td>\n      <td>3199.2</td>\n      <td>...</td>\n      <td>6764.1</td>\n      <td>4009.2</td>\n      <td>1625.4</td>\n      <td>1145.6</td>\n      <td>2402.5</td>\n      <td>48435.2</td>\n      <td>3778.1</td>\n      <td>9605.9</td>\n      <td>1885.8</td>\n      <td>3186.2</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0</td>\n      <td>58</td>\n      <td>677.8</td>\n      <td>1253.6</td>\n      <td>688.6</td>\n      <td>750.3</td>\n      <td>1948</td>\n      <td>507.3</td>\n      <td>704.2</td>\n      <td>2572.5</td>\n      <td>...</td>\n      <td>6034.1</td>\n      <td>5914</td>\n      <td>1929.2</td>\n      <td>1342.7</td>\n      <td>1799.8</td>\n      <td>45860.9</td>\n      <td>2672.2</td>\n      <td>3800.4</td>\n      <td>1750.3</td>\n      <td>3550.3</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0</td>\n      <td>60</td>\n      <td>743</td>\n      <td>1500.2</td>\n      <td>585.4</td>\n      <td>1040.6</td>\n      <td>1799.1</td>\n      <td>463.1</td>\n      <td>835.6</td>\n      <td>2541.1</td>\n      <td>...</td>\n      <td>6467.4</td>\n      <td>4185.6</td>\n      <td>1874</td>\n      <td>3359.3</td>\n      <td>1880.5</td>\n      <td>52718.7</td>\n      <td>3097.6</td>\n      <td>4902.2</td>\n      <td>1748.5</td>\n      <td>4637.3</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0</td>\n      <td>37</td>\n      <td>928.8</td>\n      <td>1863.1</td>\n      <td>772.8</td>\n      <td>1133.9</td>\n      <td>1680.7</td>\n      <td>511.9</td>\n      <td>910</td>\n      <td>2970.6</td>\n      <td>...</td>\n      <td>5923.6</td>\n      <td>10886.3</td>\n      <td>1852.3</td>\n      <td>1297.9</td>\n      <td>2176.9</td>\n      <td>38596.4</td>\n      <td>3519.6</td>\n      <td>3952</td>\n      <td>1704.6</td>\n      <td>3776.5</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0</td>\n      <td>33</td>\n      <td>900.1</td>\n      <td>1634.2</td>\n      <td>637.6</td>\n      <td>956.8</td>\n      <td>1785</td>\n      <td>500.3</td>\n      <td>760.6</td>\n      <td>2850.9</td>\n      <td>...</td>\n      <td>5605.2</td>\n      <td>3474.9</td>\n      <td>1714.7</td>\n      <td>1703.5</td>\n      <td>1969.5</td>\n      <td>42356.8</td>\n      <td>3733.1</td>\n      <td>2285.8</td>\n      <td>1694.6</td>\n      <td>4222.4</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0</td>\n      <td>58</td>\n      <td>864.1</td>\n      <td>2405.4</td>\n      <td>639</td>\n      <td>1066</td>\n      <td>1595.8</td>\n      <td>988.7</td>\n      <td>772.8</td>\n      <td>2152.8</td>\n      <td>...</td>\n      <td>6056.9</td>\n      <td>6305.9</td>\n      <td>2083.7</td>\n      <td>1119.5</td>\n      <td>1857.5</td>\n      <td>49424.6</td>\n      <td>2652.5</td>\n      <td>5989.3</td>\n      <td>1520.8</td>\n      <td>3392.1</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0</td>\n      <td>47</td>\n      <td>679.9</td>\n      <td>1500</td>\n      <td>620.4</td>\n      <td>694.9</td>\n      <td>1651.7</td>\n      <td>744.7</td>\n      <td>718.5</td>\n      <td>2276.6</td>\n      <td>...</td>\n      <td>5916.9</td>\n      <td>3472.5</td>\n      <td>1649.8</td>\n      <td>1044.3</td>\n      <td>1778.9</td>\n      <td>44215.9</td>\n      <td>3641.3</td>\n      <td>4177.6</td>\n      <td>1721.1</td>\n      <td>3827.5</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>1</td>\n      <td>61</td>\n      <td>1162.2</td>\n      <td>53296</td>\n      <td>601.8</td>\n      <td>994.2</td>\n      <td>1474.2</td>\n      <td>538.4</td>\n      <td>717.4</td>\n      <td>2502.5</td>\n      <td>...</td>\n      <td>7288.2</td>\n      <td>3786.9</td>\n      <td>1457.8</td>\n      <td>1583.4</td>\n      <td>1700.9</td>\n      <td>45366.5</td>\n      <td>3757</td>\n      <td>21388.2</td>\n      <td>1863.4</td>\n      <td>5451.3</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>1</td>\n      <td>49</td>\n      <td>709.7</td>\n      <td>1482.4</td>\n      <td>626.6</td>\n      <td>868.2</td>\n      <td>1577.9</td>\n      <td>581.4</td>\n      <td>728.6</td>\n      <td>2578.3</td>\n      <td>...</td>\n      <td>6081.3</td>\n      <td>3703.1</td>\n      <td>1488</td>\n      <td>990.7</td>\n      <td>1771.7</td>\n      <td>45773.7</td>\n      <td>2360.2</td>\n      <td>21346.6</td>\n      <td>1855.4</td>\n      <td>3633.7</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>1</td>\n      <td>60</td>\n      <td>788.5</td>\n      <td>1851.1</td>\n      <td>642.4</td>\n      <td>988.8</td>\n      <td>1627.8</td>\n      <td>481.6</td>\n      <td>783.4</td>\n      <td>2539.7</td>\n      <td>...</td>\n      <td>7812.4</td>\n      <td>4276.3</td>\n      <td>1421.4</td>\n      <td>1218.3</td>\n      <td>2874.3</td>\n      <td>56581</td>\n      <td>4822.7</td>\n      <td>48783.8</td>\n      <td>2026.7</td>\n      <td>5097.2</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0</td>\n      <td>36</td>\n      <td>897.3</td>\n      <td>1449.2</td>\n      <td>627.9</td>\n      <td>889.8</td>\n      <td>1638.4</td>\n      <td>564.1</td>\n      <td>812.6</td>\n      <td>2738.4</td>\n      <td>...</td>\n      <td>6172.8</td>\n      <td>3961.7</td>\n      <td>2082.8</td>\n      <td>1090.8</td>\n      <td>2097.9</td>\n      <td>54538.3</td>\n      <td>2898</td>\n      <td>17633.6</td>\n      <td>1726.3</td>\n      <td>3608.7</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>1</td>\n      <td>20</td>\n      <td>837.7</td>\n      <td>1834.8</td>\n      <td>624.6</td>\n      <td>1117.5</td>\n      <td>1621.6</td>\n      <td>636.9</td>\n      <td>777.7</td>\n      <td>3005.5</td>\n      <td>...</td>\n      <td>5988.3</td>\n      <td>9247.1</td>\n      <td>2016.1</td>\n      <td>1481.9</td>\n      <td>1887</td>\n      <td>39724.2</td>\n      <td>2677.1</td>\n      <td>3087.8</td>\n      <td>1818.9</td>\n      <td>2859.6</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>1</td>\n      <td>25</td>\n      <td>700.8</td>\n      <td>1472.5</td>\n      <td>577</td>\n      <td>786.1</td>\n      <td>1721.4</td>\n      <td>565.4</td>\n      <td>856.9</td>\n      <td>2410.6</td>\n      <td>...</td>\n      <td>5619.7</td>\n      <td>3805</td>\n      <td>1560.7</td>\n      <td>1235.4</td>\n      <td>1752.2</td>\n      <td>40213.1</td>\n      <td>2377.3</td>\n      <td>4907</td>\n      <td>1610.4</td>\n      <td>3660.6</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>1</td>\n      <td>44</td>\n      <td>969.6</td>\n      <td>1874.2</td>\n      <td>742.3</td>\n      <td>1463.5</td>\n      <td>2034.3</td>\n      <td>355</td>\n      <td>879.7</td>\n      <td>3275.4</td>\n      <td>...</td>\n      <td>8102.4</td>\n      <td>5301.9</td>\n      <td>2122.5</td>\n      <td>1309.5</td>\n      <td>2482.7</td>\n      <td>35540</td>\n      <td>4436.2</td>\n      <td>2806.8</td>\n      <td>1858.7</td>\n      <td>4646.6</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>1</td>\n      <td>55</td>\n      <td>1300.5</td>\n      <td>1779.2</td>\n      <td>594.5</td>\n      <td>1086.7</td>\n      <td>1703.1</td>\n      <td>483.2</td>\n      <td>815</td>\n      <td>2972.1</td>\n      <td>...</td>\n      <td>7766.7</td>\n      <td>4094.5</td>\n      <td>1591.8</td>\n      <td>1293.6</td>\n      <td>2905.1</td>\n      <td>46553.6</td>\n      <td>3333.2</td>\n      <td>8698.4</td>\n      <td>1814.7</td>\n      <td>3545.6</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>1</td>\n      <td>54</td>\n      <td>714.3</td>\n      <td>1585.6</td>\n      <td>613.4</td>\n      <td>790.9</td>\n      <td>1744</td>\n      <td>531.9</td>\n      <td>725.3</td>\n      <td>2556.4</td>\n      <td>...</td>\n      <td>6235.2</td>\n      <td>7875.5</td>\n      <td>1788</td>\n      <td>1166.5</td>\n      <td>1734.4</td>\n      <td>56901.8</td>\n      <td>2933</td>\n      <td>3095.8</td>\n      <td>1647.7</td>\n      <td>3636.7</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>1</td>\n      <td>61</td>\n      <td>858.9</td>\n      <td>3104.6</td>\n      <td>617</td>\n      <td>670.6</td>\n      <td>1632.9</td>\n      <td>698.4</td>\n      <td>692.9</td>\n      <td>2403.1</td>\n      <td>...</td>\n      <td>5100.5</td>\n      <td>4934</td>\n      <td>2671.3</td>\n      <td>993.3</td>\n      <td>1657</td>\n      <td>48410.6</td>\n      <td>3788.2</td>\n      <td>142773.7</td>\n      <td>1790.3</td>\n      <td>4445.6</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0</td>\n      <td>50</td>\n      <td>841.2</td>\n      <td>1508.9</td>\n      <td>707.6</td>\n      <td>975.1</td>\n      <td>1639.9</td>\n      <td>622.7</td>\n      <td>796.6</td>\n      <td>2637.8</td>\n      <td>...</td>\n      <td>6172.2</td>\n      <td>4471</td>\n      <td>2001.1</td>\n      <td>1098.5</td>\n      <td>2688.2</td>\n      <td>44163.6</td>\n      <td>3242.4</td>\n      <td>7225.3</td>\n      <td>2030.3</td>\n      <td>3743.1</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>1</td>\n      <td>23</td>\n      <td>773</td>\n      <td>3024.4</td>\n      <td>591.8</td>\n      <td>1278.2</td>\n      <td>1610.7</td>\n      <td>876.9</td>\n      <td>808.9</td>\n      <td>3087.8</td>\n      <td>...</td>\n      <td>5618</td>\n      <td>4033</td>\n      <td>1820.5</td>\n      <td>680.1</td>\n      <td>2156.1</td>\n      <td>41531.7</td>\n      <td>4145.6</td>\n      <td>19229.4</td>\n      <td>1977.3</td>\n      <td>2998.3</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>1</td>\n      <td>56</td>\n      <td>852.7</td>\n      <td>1313</td>\n      <td>661.5</td>\n      <td>720.4</td>\n      <td>1806.2</td>\n      <td>468.7</td>\n      <td>727.4</td>\n      <td>2516.1</td>\n      <td>...</td>\n      <td>6582.5</td>\n      <td>3745.9</td>\n      <td>1578.6</td>\n      <td>1473.2</td>\n      <td>1662</td>\n      <td>51701.8</td>\n      <td>3254.9</td>\n      <td>3779.2</td>\n      <td>2227.7</td>\n      <td>5992.9</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>1</td>\n      <td>57</td>\n      <td>649.6</td>\n      <td>1724.4</td>\n      <td>624.5</td>\n      <td>1025.8</td>\n      <td>1721.8</td>\n      <td>505</td>\n      <td>763.9</td>\n      <td>2858.1</td>\n      <td>...</td>\n      <td>6752.4</td>\n      <td>3837.5</td>\n      <td>1771.3</td>\n      <td>1022.3</td>\n      <td>2250</td>\n      <td>54806.9</td>\n      <td>4995.3</td>\n      <td>4240.4</td>\n      <td>1921.3</td>\n      <td>3950.2</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>0</td>\n      <td>58</td>\n      <td>650.2</td>\n      <td>1617</td>\n      <td>614.4</td>\n      <td>797.9</td>\n      <td>1612.5</td>\n      <td>622.2</td>\n      <td>723.8</td>\n      <td>2502.2</td>\n      <td>...</td>\n      <td>6249.1</td>\n      <td>4355.9</td>\n      <td>1701.2</td>\n      <td>907.1</td>\n      <td>1951.4</td>\n      <td>58326.4</td>\n      <td>3836.4</td>\n      <td>9081.7</td>\n      <td>1655.2</td>\n      <td>3463.9</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>1</td>\n      <td>38</td>\n      <td>609.5</td>\n      <td>1388.3</td>\n      <td>662.7</td>\n      <td>910</td>\n      <td>1774.1</td>\n      <td>482.9</td>\n      <td>758.7</td>\n      <td>2526.7</td>\n      <td>...</td>\n      <td>7033.5</td>\n      <td>4095.3</td>\n      <td>1573.3</td>\n      <td>1098</td>\n      <td>2186.5</td>\n      <td>51337.6</td>\n      <td>3890.2</td>\n      <td>3423.9</td>\n      <td>1725.3</td>\n      <td>3330.7</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>0</td>\n      <td>23</td>\n      <td>1025.2</td>\n      <td>1962.9</td>\n      <td>764</td>\n      <td>1088.3</td>\n      <td>1548.9</td>\n      <td>728</td>\n      <td>1052.4</td>\n      <td>2730.1</td>\n      <td>...</td>\n      <td>6593.1</td>\n      <td>4653.6</td>\n      <td>1908.3</td>\n      <td>1251.7</td>\n      <td>1453.6</td>\n      <td>44700.7</td>\n      <td>2814.2</td>\n      <td>2007.7</td>\n      <td>2067.9</td>\n      <td>3066.1</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0</td>\n      <td>44</td>\n      <td>849.7</td>\n      <td>1431.6</td>\n      <td>547.9</td>\n      <td>819.9</td>\n      <td>1608.7</td>\n      <td>539.3</td>\n      <td>767.7</td>\n      <td>2252.1</td>\n      <td>...</td>\n      <td>5820.4</td>\n      <td>4006.9</td>\n      <td>1722.6</td>\n      <td>1418</td>\n      <td>1538.3</td>\n      <td>46719.5</td>\n      <td>4732.5</td>\n      <td>2237</td>\n      <td>1527</td>\n      <td>3751.5</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>1</td>\n      <td>56</td>\n      <td>1189.4</td>\n      <td>1612.9</td>\n      <td>646.6</td>\n      <td>923.6</td>\n      <td>1585.6</td>\n      <td>469.7</td>\n      <td>803.1</td>\n      <td>2530.2</td>\n      <td>...</td>\n      <td>5957.5</td>\n      <td>3826</td>\n      <td>2090.9</td>\n      <td>1462.9</td>\n      <td>1576</td>\n      <td>45429.9</td>\n      <td>3838</td>\n      <td>12676.7</td>\n      <td>1950.6</td>\n      <td>4877.9</td>\n    </tr>\n  </tbody>\n</table>\n<p>42 rows × 1319 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\n# instantiate a distribution strategy\ntf.tpu.experimental.initialize_tpu_system(tpu)\ntpu_strategy = tf.distribute.TPUStrategy(tpu)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T20:07:21.195741Z","iopub.execute_input":"2024-05-15T20:07:21.196113Z","iopub.status.idle":"2024-05-15T20:07:21.338294Z","shell.execute_reply.started":"2024-05-15T20:07:21.196085Z","shell.execute_reply":"2024-05-15T20:07:21.336914Z"},"trusted":true},"execution_count":8,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# detect and init the TPU\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tpu \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcluster_resolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTPUClusterResolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# instantiate a distribution strategy\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tf\u001b[38;5;241m.\u001b[39mtpu\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39minitialize_tpu_system(tpu)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py:235\u001b[0m, in \u001b[0;36mTPUClusterResolver.__init__\u001b[0;34m(self, tpu, zone, project, job_name, coordinator_name, coordinator_address, credentials, service, discovery_url)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new TPUClusterResolver object.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03mThe ClusterResolver will then use the parameters to query the Cloud TPU APIs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m    Google Cloud environment.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tpu \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    234\u001b[0m   \u001b[38;5;66;03m# Default Cloud environment\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cloud_tpu_client \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m      \u001b[49m\u001b[43mzone\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzone\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m      \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m      \u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdiscovery_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscovery_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tpu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cloud_tpu_client\u001b[38;5;241m.\u001b[39mname()\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m   \u001b[38;5;66;03m# Directly connected TPU environment\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/cloud_tpu_client/client.py:139\u001b[0m, in \u001b[0;36mClient.__init__\u001b[0;34m(self, tpu, zone, project, credentials, service, discovery_url)\u001b[0m\n\u001b[1;32m    136\u001b[0m tpu \u001b[38;5;241m=\u001b[39m _get_tpu_name(tpu)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tpu \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease provide a TPU Name to connect to.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tpu \u001b[38;5;241m=\u001b[39m _as_text(tpu)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_api \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tpu\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrpc://\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mValueError\u001b[0m: Please provide a TPU Name to connect to."],"ename":"ValueError","evalue":"Please provide a TPU Name to connect to.","output_type":"error"}]},{"cell_type":"code","source":"from scikeras.wrappers import KerasRegressor\n\n# create a function that will build and compile a Keras model\n\ndef NN_builder(n_hidden=1, optimizer = 'rmsprop', units=40, learning_rate = 0.001, input_shape=[17], \n               regularization=0.001, dropout=0.2, activation = 'sigmoid'):\n    #with tpu_strategy.scope():\n        model = keras.models.Sequential()\n        model.add(keras.layers.InputLayer(input_shape=input_shape))\n\n        for layer in range (n_hidden):\n            model.add(keras.layers.Dense(units=40, activation=activation, activity_regularizer=l1_l2(regularization)))\n            model.add(Dropout(dropout))\n\n        model.add(keras.layers.Dense(units=1,activation='sigmoid'))\n        optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n\n        model.compile(loss=\"mean_absolute_error\", optimizer=optimizer)\n        return model\n\n#NN = KerasRegressor(model=NN_builder,n_hidden=1, optimizer = 'rmsprop', units=40, learning_rate = 0.001, input_shape=[17], \n               #regularization=0.001, dropout=0.2, activation = 'sigmoid')\nNN = KerasRegressor(NN_builder)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"83ffffa4","outputId":"ef0e5b7b-4d3e-4714-afc5-eeb6d511e94c","execution":{"iopub.status.busy":"2024-05-15T20:20:06.407693Z","iopub.execute_input":"2024-05-15T20:20:06.408124Z","iopub.status.idle":"2024-05-15T20:20:06.908414Z","shell.execute_reply.started":"2024-05-15T20:20:06.408092Z","shell.execute_reply":"2024-05-15T20:20:06.906949Z"},"trusted":true},"execution_count":21,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscikeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasRegressor\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# create a function that will build and compile a Keras model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mNN_builder\u001b[39m(n_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m, units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m, learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m17\u001b[39m], \n\u001b[1;32m      6\u001b[0m                regularization\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#with tpu_strategy.scope():\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scikeras/wrappers.py:35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscikeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m loss_name, metric_name\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscikeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom_state\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensorflow_random_state\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscikeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClassifierLabelEncoder, RegressorTargetEncoder\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseWrapper\u001b[39;00m(BaseEstimator):\n\u001b[1;32m     39\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implementation of the scikit-learn classifier API for Keras.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    Below are a list of SciKeras specific parameters. For details on other parameters,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m        The number of features seen during `fit`.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scikeras/utils/transformers.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseEstimator, TransformerMixin\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NotFittedError\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_pipeline\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionTransformer, OneHotEncoder, OrdinalEncoder\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m type_of_target\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sparse\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransformerMixin, _fit_context, clone\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NotFittedError\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionTransformer\n","\u001b[0;31mImportError\u001b[0m: cannot import name '_fit_context' from 'sklearn.base' (/opt/conda/lib/python3.10/site-packages/sklearn/base.py)"],"ename":"ImportError","evalue":"cannot import name '_fit_context' from 'sklearn.base' (/opt/conda/lib/python3.10/site-packages/sklearn/base.py)","output_type":"error"}]},{"cell_type":"code","source":"!pip install scikeras","metadata":{"execution":{"iopub.status.busy":"2024-05-15T20:19:10.369615Z","iopub.execute_input":"2024-05-15T20:19:10.370026Z","iopub.status.idle":"2024-05-15T20:19:29.017209Z","shell.execute_reply.started":"2024-05-15T20:19:10.369990Z","shell.execute_reply":"2024-05-15T20:19:29.016117Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Collecting scikeras\n  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\nRequirement already satisfied: keras>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from scikeras) (3.3.3)\nCollecting scikit-learn>=1.4.2 (from scikeras)\n  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras>=3.2.0->scikeras) (1.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras>=3.2.0->scikeras) (1.26.4)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.2.0->scikeras) (13.7.0)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.2.0->scikeras) (0.0.8)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras>=3.2.0->scikeras) (3.10.0)\nRequirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.2.0->scikeras) (0.11.0)\nRequirement already satisfied: ml-dtypes in /opt/conda/lib/python3.10/site-packages (from keras>=3.2.0->scikeras) (0.2.0)\nRequirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.4.2->scikeras) (1.11.4)\nRequirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.4.2->scikeras) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.4.2->scikeras) (3.2.0)\nRequirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from optree->keras>=3.2.0->scikeras) (4.9.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.2.0->scikeras) (2.17.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\nDownloading scikeras-0.13.0-py3-none-any.whl (26 kB)\nDownloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: scikit-learn, scikeras\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed scikeras-0.13.0 scikit-learn-1.4.2\n","output_type":"stream"}]},{"cell_type":"code","source":"model = NN\np_grid ={\"n_hidden\" : [1,2,3,4,5],\n         \"units\" : [10,20,30,40,50],\n         \"learning_rate\": [0.0001,0.001,0.01],\n         \"regularization\":[1e-2,1e-3,1e-4],\n         \"dropout\":[0.0,0.1,0.2,0.3],\n         \"batch_size\":[32, 64, 128, 256],\n         \"activation\": ['relu', 'tanh', 'sigmoid']}\n\nxArr.columns = xArr.columns.astype(str)\nstdScale=StandardScaler().fit(xArr)\nX=stdScale.transform(xArr)\nY = yArr.astype('int')","metadata":{"id":"729e6604","execution":{"iopub.status.busy":"2024-05-15T20:07:28.078266Z","iopub.execute_input":"2024-05-15T20:07:28.078967Z","iopub.status.idle":"2024-05-15T20:07:28.131534Z","shell.execute_reply.started":"2024-05-15T20:07:28.078934Z","shell.execute_reply":"2024-05-15T20:07:28.130192Z"},"trusted":true},"execution_count":10,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrappers\u001b[49m\u001b[38;5;241m.\u001b[39mscikit_learn\u001b[38;5;241m.\u001b[39mKerasRegressor(NN_builder)\n\u001b[1;32m      2\u001b[0m p_grid \u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_hidden\u001b[39m\u001b[38;5;124m\"\u001b[39m : [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m],\n\u001b[1;32m      3\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munits\u001b[39m\u001b[38;5;124m\"\u001b[39m : [\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m40\u001b[39m,\u001b[38;5;241m50\u001b[39m],\n\u001b[1;32m      4\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m0.0001\u001b[39m,\u001b[38;5;241m0.001\u001b[39m,\u001b[38;5;241m0.01\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m:[\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m],\n\u001b[1;32m      8\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m     10\u001b[0m xArr\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m xArr\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.keras' has no attribute 'wrappers'"],"ename":"AttributeError","evalue":"module 'tensorflow.keras' has no attribute 'wrappers'","output_type":"error"}]},{"cell_type":"code","source":"from tensorflow.keras.layers import  Dropout\nfrom tensorflow.keras.regularizers import l1_l2\nfrom tensorflow.keras.layers import Dense,Dropout\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Number of epochs\nepochs = 50\n\n# Define callaback\ncallbacks=[keras.callbacks.EarlyStopping(monitor='loss', patience=5)]\n\nNUM_TRIALS = 10\n\nitr_number = [] # create new empty list for itr number \nouter_results = []\ninner_results = []\nmodel_params = []\ny_test_list = []\npred_list = []\npred_var_list = []\n\nfor i in range(NUM_TRIALS): #configure the cross-validation procedure - outer loop (test set) \n\n      cv_outer = ShuffleSplit(n_splits=1, test_size=0.2, random_state=i) #hold back 20% of the groups for test set\n\n      # split data using GSS\n      for train_index, test_index in cv_outer.split(X, Y):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = Y[train_index], Y[test_index]\n\n        # store test set information\n        y_test = np.array(y_test) #prevents index from being brought from dataframe\n        y_test_list.append(y_test)\n\n        # configure the cross-validation procedure - inner loop (validation set/HP optimization)\n        cv_inner = KFold(n_splits=10) #should be 10 fold group split for inner loop\n\n        # define search space\n        search = RSCV(model, p_grid, n_iter=50, verbose=3, scoring='neg_mean_absolute_error', cv=cv_inner, refit=True) # should be 100\n\n        # execute search\n        result = search.fit(X_train, y_train, callbacks=callbacks, epochs=epochs, verbose=0)\n\n        # get the best performing model fit on the whole training set\n        best_model = result.best_estimator_\n\n        # get the score for the best performing model and store\n        best_score = abs(result.best_score_)\n        inner_results.append(best_score)\n\n        # evaluate model and estimate epistemic uncertainty on the hold out dataset\n        predictions = []\n        for _ in range(100):\n            predictions += [best_model.predict(X_test, verbose=0)]\n        \n        yhat, pred_unbiased = np.mean(np.array(predictions), axis=0), np.std(np.array(predictions), axis=0)\n\n        # store drug release predictions\n        pred_list.append(yhat)\n            \n        # store prediction variance\n        pred_var_list.append(pred_unbiased)\n\n        # evaluate the model\n        acc = mean_absolute_error(y_test, yhat)\n\n        # store the result\n        itr_number.append(i+1)\n        outer_results.append(acc)\n        model_params.append(result.best_params_)\n\n      # report progress at end of each inner loop\n      print('\\n################################################################\\n\\nSTATUS REPORT:') \n      print('Iteration '+str(i+1)+' of '+str(NUM_TRIALS)+' runs completed') \n      print('Test_Score: %.3f, Best_Valid_Score: %.3f, \\n\\nBest_Model_Params: \\n%s' % (acc, best_score, result.best_params_))\n      print(\"\\n################################################################\\n \")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"046f712a","outputId":"aa91b5f6-daf5-4e0e-d307-349707befc1c","execution":{"iopub.status.busy":"2024-05-15T19:58:43.955766Z","iopub.execute_input":"2024-05-15T19:58:43.956061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create dataframe with results of nested CV\nlist_of_tuples = list(zip(itr_number, inner_results, outer_results, model_params, G_test_list, E_test_list, T_test_list, y_test_list, pred_list, pred_var_list))\nCV_dataset = pd.DataFrame(list_of_tuples, columns = ['Iter', 'Valid Score', 'Test Score', 'Model Parms', 'DP_Groups', \"Experimental Index\", \"Time\", 'Experimental_Release', 'Predicted_Release','Prediction_Variance'])\nCV_dataset['Score_difference'] = abs(CV_dataset['Valid Score'] - CV_dataset['Test Score']) #Groupby dataframe model iterations that best fit the data (i.e., validitaion <= test)\nCV_dataset.sort_values(by=['Score_difference', 'Test Score'], ascending=True, inplace=True) \nCV_dataset = CV_dataset.reset_index(drop=True) # Reset index of dataframe\nCV_dataset.to_pickle(\"/kaggle/working/NN.pkl\") # save dataframe as pickle file\n","metadata":{"id":"c5dc67d9","execution":{"iopub.status.busy":"2024-05-15T19:58:52.960849Z","iopub.execute_input":"2024-05-15T19:58:52.961179Z","iopub.status.idle":"2024-05-15T19:58:53.193841Z","shell.execute_reply.started":"2024-05-15T19:58:52.961137Z","shell.execute_reply":"2024-05-15T19:58:53.192894Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#create dataframe with results of nested CV\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m list_of_tuples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[43mitr_number\u001b[49m, inner_results, outer_results, model_params, G_test_list, E_test_list, T_test_list, y_test_list, pred_list, pred_var_list))\n\u001b[1;32m      3\u001b[0m CV_dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(list_of_tuples, columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValid Score\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Score\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel Parms\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDP_Groups\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExperimental Index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExperimental_Release\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted_Release\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrediction_Variance\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m CV_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScore_difference\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(CV_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValid Score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m CV_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Score\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;66;03m#Groupby dataframe model iterations that best fit the data (i.e., validitaion <= test)\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'itr_number' is not defined"],"ename":"NameError","evalue":"name 'itr_number' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import pickle\n# assign the best model paramaters\nbest_model_params = CV_dataset.iloc[0,3]\n# set params from the best model to a class object\nbest_model = model.set_params(**best_model_params)\nbest_model = best_model.fit(X, Y)\nwith open('/kaggle/working/NN_model.pkl', 'wb') as file: # Save the Model to pickle file\n          pickle.dump(best_model, file)","metadata":{"id":"95c34417","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CV_dataset.describe()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"a6718924","outputId":"b881f3e3-3254-43c6-aaa1-60e12d91e231","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"tzZmTfy0y8rn"},"execution_count":null,"outputs":[]}]}